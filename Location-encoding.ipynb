{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cddd186-c993-4ae6-a9bc-66fd8cbdcf58",
   "metadata": {},
   "source": [
    "# Location Encoding: An Introduction\n",
    "\n",
    "In a world where geographic information is fundamental to navigation, mapping, and data analysis, **location encoding systems** play a crucial role in structuring and simplifying spatial data. Traditional latitude and longitude coordinates provide a precise way to identify any location on Earth, but alternative encoding systems have emerged to enhance usability, efficiency, and integration into digital applications.\n",
    "\n",
    "From **geospatial databases and logistics optimization** to **emergency response and machine learning**, different encoding methods allow for faster indexing, more human-friendly location representation, and scalable spatial analysis. Systems like **Geohash, H3, Plus Codes, UTM, and coordinate projections** transform raw spatial data into structured formats that improve accuracy and retrieval speed.\n",
    "\n",
    "### Why Do We Need Location Encoding?\n",
    "1. **Efficient Search & Indexing** – Many systems store and retrieve geographic data faster using structured encodings instead of raw coordinates.\n",
    "2. **Scalability** – Global applications like Uber and Google Maps require spatial partitioning techniques to handle millions of locations efficiently.\n",
    "3. **Better Human Interaction** – Some encodings replace complex lat/lon values with easy-to-read codes, improving usability in real-world navigation.\n",
    "4. **Machine Learning & AI** – Encoding methods allow geospatial models to learn from structured spatial data, improving predictions in mapping applications.\n",
    "\n",
    "### What This Lesson Covers\n",
    "In this lesson, we will explore:\n",
    "- The fundamental **logic behind different encoding systems**\n",
    "- **How location encoding improves spatial applications**\n",
    "- **Hands-on demonstrations** using Python to encode and decode location data\n",
    "- **How TorchSpatial can be leveraged in machine learning for geospatial tasks**\n",
    "\n",
    "By the end of this lesson, you will understand why encoding systems exist, how they work, and how they improve real-world applications involving geographic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e7266a9-16da-4e4b-a81b-8fbd26601d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: geohash2 in /home/jovyan/.local/geoai/lib/python3.11/site-packages (1.1)\n",
      "Requirement already satisfied: h3 in /home/jovyan/.local/geoai/lib/python3.11/site-packages (4.2.1)\n",
      "Requirement already satisfied: pluscodes in /home/jovyan/.local/geoai/lib/python3.11/site-packages (2022.1.3)\n",
      "Requirement already satisfied: utm in /home/jovyan/.local/geoai/lib/python3.11/site-packages (0.8.0)\n",
      "Requirement already satisfied: pyproj in /cvmfs/iguide.purdue.edu/software/conda/geoai/lib/python3.11/site-packages (3.6.1)\n",
      "Requirement already satisfied: docutils>=0.3 in /home/jovyan/.local/geoai/lib/python3.11/site-packages (from geohash2) (0.21.2)\n",
      "Requirement already satisfied: certifi in /cvmfs/iguide.purdue.edu/software/conda/geoai/lib/python3.11/site-packages (from pyproj) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "# Installing Required Libraries\n",
    "!pip install geohash2 h3 pluscodes utm pyproj "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef25485e-228a-4c73-b605-b8407fd88b80",
   "metadata": {},
   "source": [
    "## 1. Understanding Different Location Encoding Methods\n",
    "\n",
    "Below, we will explore five major location encoding systems, explaining the logic behind them and providing code implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c48217-ced7-4a5b-8b6a-0d313af985ed",
   "metadata": {},
   "source": [
    "### 1.1. Geohash Encoding\n",
    "\n",
    "Geohash encodes geographic coordinates into a short alphanumeric string by dividing the Earth's surface into a grid. Each level of precision refines the grid into smaller subgrids.\n",
    "\n",
    "- The world is first divided into a base 32 grid (longitude-first binary subdivision).\n",
    "- Each subdivision refines the location further, producing a unique identifier.\n",
    "- Longer geohashes = More precision (finer subdivisions).\n",
    "\n",
    "#### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2413db55-2d74-4dea-8e81-7af6938c154e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geohash: dr5regw\n",
      "Decoded Coordinates: 40.71, -74.01\n"
     ]
    }
   ],
   "source": [
    "import geohash2\n",
    "\n",
    "latitude = 40.7128  # New York City\n",
    "longitude = -74.0060\n",
    "\n",
    "# Encode to geohash\n",
    "geohash_code = geohash2.encode(latitude, longitude, precision=7)\n",
    "print(f\"Geohash: {geohash_code}\")\n",
    "\n",
    "# Decode geohash back to coordinates\n",
    "decoded_lat, decoded_lon = geohash2.decode(geohash_code)\n",
    "print(f\"Decoded Coordinates: {decoded_lat}, {decoded_lon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4898634-e644-4b32-8345-28b3cc7ec984",
   "metadata": {},
   "source": [
    "### 1.2. H3 Hexagonal Encoding\n",
    "\n",
    "H3 is a hexagonal hierarchical spatial index created by Uber. Unlike traditional square grids, hexagons minimize distortions in spatial computations.\n",
    "\n",
    "- The Earth is projected onto an icosahedron (a 20-sided shape).\n",
    "- Each face is subdivided into hexagons at different levels (resolution 0-15).\n",
    "- The hexagon structure helps smooth spatial relationships for ML models.\n",
    "\n",
    "#### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "914d84a1-db73-46af-882d-d2f288e1f450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H3 Code: 892a1072893ffff\n",
      "H3 Cell Center: (40.71237820442782, -74.0056425771711)\n"
     ]
    }
   ],
   "source": [
    "import h3\n",
    "\n",
    "latitude = 40.7128\n",
    "longitude = -74.0060\n",
    "\n",
    "# Convert lat/lon to H3 index (resolution 9 for medium precision)\n",
    "h3_code = h3.latlng_to_cell(latitude, longitude, 9)\n",
    "print(f\"H3 Code: {h3_code}\")\n",
    "\n",
    "# Get the center of the H3 cell\n",
    "h3_center = h3.cell_to_latlng(h3_code)\n",
    "print(f\"H3 Cell Center: {h3_center}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91114686-c5ae-4d10-9d0f-07a1793d0269",
   "metadata": {},
   "source": [
    "### 1.3. Plus Codes (Open Location Code by Google)\n",
    "\n",
    "Google’s Plus Codes encode locations into a 10-character alphanumeric string, eliminating the need for street addresses.\n",
    "\n",
    "- The world is divided into 4° x 5° blocks (each assigned a unique code).\n",
    "- Sub-blocks are refined into smaller sections, increasing precision.\n",
    "- Can be shortened locally if the area is known (e.g., 8QQ7+V8, New York).\n",
    "\n",
    "#### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36db7754-1f6e-4123-bc7d-3915636110b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plus Code: 87G7PX7V+4J\n",
      "Decoded Coordinates: Area(sw=Point(lat=40.71275, lon=-74.006), ne=Point(lat=40.712875, lon=-74.005875))\n"
     ]
    }
   ],
   "source": [
    "import pluscodes\n",
    "\n",
    "latitude = 40.7128\n",
    "longitude = -74.0060\n",
    "\n",
    "# Encode latitude/longitude to Plus Code\n",
    "plus_code = pluscodes.encode(latitude, longitude)\n",
    "print(f\"Plus Code: {plus_code}\")\n",
    "\n",
    "# Decode Plus Code back to latitude/longitude\n",
    "decoded_latlon = pluscodes.decode(plus_code)\n",
    "print(f\"Decoded Coordinates: {decoded_latlon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364ca849-f246-4ecd-8c34-a4997514fc74",
   "metadata": {},
   "source": [
    "### 1.4. UTM (Universal Transverse Mercator) Projection\n",
    "\n",
    "UTM converts geographic coordinates into a Cartesian (X, Y) grid, making distance calculations more accurate.\n",
    "\n",
    "- The Earth is divided into 60 longitudinal zones (each 6° wide).\n",
    "- Coordinates are expressed in meters instead of degrees.\n",
    "- Removes distortions of curved coordinates, useful for surveying and mapping.\n",
    "\n",
    "#### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6920d931-240f-408a-8a43-60ad00ac3688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTM Coordinates: (583959.3723242896, 4507350.998362971, 18, 'T')\n",
      "Decoded Coordinates: (40.712800000511166, -74.00599999999046)\n"
     ]
    }
   ],
   "source": [
    "import utm\n",
    "\n",
    "latitude = 40.7128\n",
    "longitude = -74.0060\n",
    "\n",
    "# Convert lat/lon to UTM\n",
    "utm_coords = utm.from_latlon(latitude, longitude)\n",
    "print(f\"UTM Coordinates: {utm_coords}\")\n",
    "    \n",
    "# Convert UTM back to lat/lon\n",
    "latlon = utm.to_latlon(*utm_coords)\n",
    "print(f\"Decoded Coordinates: {latlon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a0f522-c03d-4a04-9c56-0aa7e68d9b58",
   "metadata": {},
   "source": [
    "### 1.5. Coordinate Projection using PyProj\n",
    "\n",
    "Different projections transform coordinates into different reference systems to match various mapping needs.\n",
    "\n",
    "- EPSG:4326 (WGS 84): Standard GPS format (lat/lon in degrees).\n",
    "- EPSG:3857 (Web Mercator): Used in Google Maps, translates lat/lon to meters.\n",
    "\n",
    "#### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b08dd7da-854c-4a70-a18f-6290db4b4920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mercator Projection: X=-8238310.235647004, Y=4970071.579142427\n"
     ]
    }
   ],
   "source": [
    "import pyproj\n",
    "\n",
    "# Define projections\n",
    "wgs84 = pyproj.Proj(\"epsg:4326\")  # Standard lat/lon\n",
    "mercator = pyproj.Proj(\"epsg:3857\")  # Web Mercator\n",
    "\n",
    "# Convert WGS84 to Mercator projection\n",
    "transformer = pyproj.Transformer.from_crs(\"epsg:4326\", \"epsg:3857\", always_xy=True)\n",
    "x, y = transformer.transform(-74.0060, 40.7128)\n",
    "\n",
    "print(f\"Mercator Projection: X={x}, Y={y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4689c73f-5f50-4ff8-a075-4bd775220f18",
   "metadata": {},
   "source": [
    "## 2. Location Encoding for Machine Learning\n",
    "Incorporating **location encoding** into **machine learning** models can significantly improve the understanding of spatial relationships. **Transformers**, originally designed for NLP, have demonstrated exceptional capabilities in learning complex dependencies in spatial and spatiotemporal data.\n",
    "\n",
    "By encoding locations into structured inputs such as **Geohash, H3, and Plus Codes**, Transformers can:\n",
    "\n",
    "- Capture **spatial dependencies** through self-attention.\n",
    "- Learn **irregular spatial patterns** without relying on fixed grids.\n",
    "- Combine **location with other features** (e.g., time, weather, mobility).\n",
    "\n",
    "This section explores how to encode geographic locations effectively and feed them into a Transformer-based model for geospatial learning tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490cf02e-541f-404f-8ea5-24ce6ed724e0",
   "metadata": {},
   "source": [
    "### 2.1. Location Encoding for Machine Learning\n",
    "Geospatial data is essential in machine learning for tasks like traffic prediction, logistics, and real estate analysis. While raw latitude and longitude can be used, location encoding improves spatial relationships, indexing, and clustering. Techniques like Geohash, H3, and Plus Codes help models better understand spatial dependencies. \n",
    "\n",
    "#### 2.1.1. Geohash Encoding\n",
    "\n",
    "- Converts lat/lon into Geohash sequences, treating them like NLP tokens.\n",
    "- Applies positional encoding to retain spatial relationships.\n",
    "\n",
    "**Example**: Encoding Geohash for Transformer Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6315e3ca-b07f-43de-aa97-0944ace6d5bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geohash: ['dr5regw', '9q5ctr1', 'gcpvj0d']\n",
      "Geohash Tokens: [1 0 2]\n",
      "Geohash Encodings: [[0.1430242  0.15125186 0.23136721 0.87048953 0.5151535  0.44272734\n",
      "  0.38185424 0.47106475]\n",
      " [0.16321233 0.30181433 0.37270195 0.30501955 0.39813302 0.74358366\n",
      "  0.79784693 0.73026866]\n",
      " [0.56680009 0.3564524  0.75445564 0.15721406 0.21585745 0.0381813\n",
      "  0.08092735 0.73499752]]\n"
     ]
    }
   ],
   "source": [
    "import geohash2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Example locations\n",
    "locations = [(40.7128, -74.0060), (34.0522, -118.2437), (51.5074, -0.1278)]  # NYC, LA, London\n",
    "\n",
    "# Convert to Geohash\n",
    "geohashes = [geohash2.encode(lat, lon, precision=7) for lat, lon in locations]\n",
    "print(f\"Geohash: {geohashes}\")\n",
    "\n",
    "# Encode Geohashes as categorical labels\n",
    "label_encoder = LabelEncoder()\n",
    "geohash_tokens = label_encoder.fit_transform(geohashes)\n",
    "print(f\"Geohash Tokens: {geohash_tokens}\")\n",
    "\n",
    "# Example of Converted embeddings\n",
    "geohash_embeddings = np.random.rand(len(geohash_tokens), 8)  # 8D embeddings\n",
    "\n",
    "print(f\"Geohash Encodings: {geohash_embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f68803-ebac-491c-933a-c79e1576647b",
   "metadata": {},
   "source": [
    "#### 2.1.2. H3 Encoding\n",
    "\n",
    "* Converts lat/lon into H3 hexagonal indices to structure spatial data.\n",
    "* Uses self-attention to capture relationships between grid cells.\n",
    "\n",
    "**Example**:  H3 Encoding for Transformer Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "436f470e-332c-4353-b679-e4e700ee51d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H3 Indices: ['872a1072cffffff', '8729a1d75ffffff', '87195da49ffffff']\n",
      "H3 Tokenized IDs: [2 1 0]\n",
      "H3 Encoded Embeddings: [[0.68584439 0.32866543 0.0113972  0.37992344 0.46947381 0.78907562\n",
      "  0.7442883  0.88290534]\n",
      " [0.94203986 0.54499083 0.82712323 0.49882867 0.57967362 0.29111199\n",
      "  0.4318877  0.52954851]\n",
      " [0.10301203 0.77339249 0.52837039 0.27743033 0.64754406 0.44779048\n",
      "  0.91101895 0.20599645]]\n"
     ]
    }
   ],
   "source": [
    "import h3\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example locations (latitude, longitude)\n",
    "locations = [(40.7128, -74.0060), (34.0522, -118.2437), (51.5074, -0.1278)]  # NYC, LA, London\n",
    "\n",
    "# Convert to H3 grid index (resolution 7)\n",
    "h3_indices = [h3.latlng_to_cell(lat, lon, 7) for lat, lon in locations]\n",
    "print(f\"H3 Indices: {h3_indices}\")\n",
    "\n",
    "# Encode H3 indices as numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "h3_tokens = label_encoder.fit_transform(h3_indices)\n",
    "\n",
    "print(f\"H3 Tokenized IDs: {h3_tokens}\")\n",
    "\n",
    "# Example of Converted embeddings\n",
    "h3_embeddings = np.random.rand(len(h3_tokens), 8)  # 8D embedding vectors\n",
    "\n",
    "print(f\"H3 Encoded Embeddings: {h3_embeddings}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4a1315-9c60-4d3f-b99f-d0a2fdf7955d",
   "metadata": {},
   "source": [
    "#### 2.1.3. Plus Code\n",
    "\n",
    "- Uses Google’s Plus Codes as structured geospatial identifiers.\n",
    "- Allows integration with temporal data for time-aware ML models.\n",
    "  \n",
    "**Example**: Plus Code Encoding for Transformer Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d713a7d-3613-4fff-b96b-1b0db124f57a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plus codes: ['87G7PX7V+4J', '85633Q24+VG', '9C3XGV4C+XV']\n",
      "Plus Code Tokens: [1 0 2]\n",
      "Plus Code Embeddings: [[0.94252751 0.10543829 0.26286068 0.08549326 0.45069973 0.0431945\n",
      "  0.88717164 0.40042766]\n",
      " [0.23439792 0.38652673 0.93544833 0.90533573 0.82676326 0.0551493\n",
      "  0.53211298 0.52761233]\n",
      " [0.87465922 0.78434456 0.69519955 0.63586629 0.87610709 0.55728817\n",
      "  0.90655374 0.39734345]]\n"
     ]
    }
   ],
   "source": [
    "import pluscodes\n",
    "import numpy as np\n",
    "\n",
    "# Example locations\n",
    "locations = [(40.7128, -74.0060), (34.0522, -118.2437), (51.5074, -0.1278)]  # NYC, LA, London\n",
    "\n",
    "# Convert to Plus Codes\n",
    "plus_codes = [pluscodes.encode(lat, lon) for lat, lon in locations]\n",
    "print(f\"Plus codes: {plus_codes}\")\n",
    "\n",
    "# Encode Plus Codes as categorical labels\n",
    "label_encoder = LabelEncoder()\n",
    "plus_code_tokens = label_encoder.fit_transform(plus_codes)\n",
    "\n",
    "print(f\"Plus Code Tokens: {plus_code_tokens}\")\n",
    "\n",
    "# Example of Converted embeddings\n",
    "plus_code_embeddings = np.random.rand(len(plus_code_tokens), 8)  # 8D embeddings\n",
    "\n",
    "print(f\"Plus Code Embeddings: {plus_code_embeddings}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9113f7f-fb00-430e-aea8-a849d49d4d74",
   "metadata": {},
   "source": [
    "## 3. Example: Traffic Prediction Using Location Encoding\n",
    "Now that we have properly encoded locations using Geohash, H3, and Plus Codes, we can integrate them into a Transformer model. Instead of using random inputs, the model will take actual encoded location data and learn spatial dependencies.\n",
    "\n",
    "### 3.1.  Generating Dummy Traffic Data\n",
    "\n",
    "We will use the tokenized encodings from our previous examples:\n",
    "\n",
    "- Geohash Encoding\n",
    "- H3 Encoding\n",
    "- Plus Codes Encoding\n",
    "\n",
    "Each encoding will be converted into numerical tokens and then passed into the Transformer model.\n",
    "\n",
    "#### Encoding Locations and Creating Tokenized Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d3763a-348d-40b1-be79-8d136c739f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Location Sequences:\n",
      "tensor([[12, 10, 13],\n",
      "        [11, 14,  5],\n",
      "        [ 4,  2,  3],\n",
      "        [ 6,  7,  1],\n",
      "        [ 9,  0,  8]])\n",
      "Traffic Density Labels:\n",
      "tensor([2, 1, 1, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "import geohash2\n",
    "import h3\n",
    "import pluscodes\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Simulated locations with traffic density labels (0 = Low, 1 = Medium, 2 = High)\n",
    "locations = [\n",
    "    (40.7128, -74.0060, 2),  # NYC (High Traffic)\n",
    "    (34.0522, -118.2437, 1),  # LA (Medium Traffic)\n",
    "    (51.5074, -0.1278, 1),  # London (Medium Traffic)\n",
    "    (37.7749, -122.4194, 2),  # SF (High Traffic)\n",
    "    (35.6895, 139.6917, 0),  # Tokyo (Low Traffic)\n",
    "]\n",
    "\n",
    "# Encode locations using Geohash, H3, and Plus Codes\n",
    "geohashes = [geohash2.encode(lat, lon, precision=7) for lat, lon, _ in locations]\n",
    "h3_indices = [h3.latlng_to_cell(lat, lon, 7) for lat, lon, _ in locations]\n",
    "plus_codes = [pluscodes.encode(lat, lon) for lat, lon, _ in locations]\n",
    "\n",
    "# Combine all encodings into a feature set\n",
    "all_tokens = geohashes + h3_indices + plus_codes\n",
    "\n",
    "# Convert encodings into numerical tokens\n",
    "label_encoder = LabelEncoder()\n",
    "tokenized_locations = label_encoder.fit_transform(all_tokens)\n",
    "\n",
    "# Reshape into sequences for the Transformer (Batch Size = 5, Sequence Length = 3 Encodings per Location)\n",
    "tokenized_locations = np.reshape(tokenized_locations, (len(locations), 3))\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "sample_input = torch.tensor(tokenized_locations, dtype=torch.long)\n",
    "\n",
    "# Traffic density labels\n",
    "traffic_labels = torch.tensor([traffic for _, _, traffic in locations], dtype=torch.long)\n",
    "\n",
    "print(f\"Tokenized Location Sequences:\\n{sample_input}\")\n",
    "print(f\"Traffic Density Labels:\\n{traffic_labels}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48627a49-511f-4655-8085-b9a1367bd261",
   "metadata": {},
   "source": [
    "### 3.2. Transformer Model for Traffic Prediction\n",
    "We now modify the Transformer model to predict traffic density at a given location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d92ff0b6-ff2e-4787-a773-07d962d3fa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/iguide.purdue.edu/software/conda/geoai/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Model Predictions (Before Training):\n",
      "tensor([[ 0.0910,  0.0782,  0.0584],\n",
      "        [-0.2801,  0.6268,  0.2978],\n",
      "        [-0.0632,  0.4625,  0.6052],\n",
      "        [-0.1840,  0.5490,  0.2076],\n",
      "        [-0.6436,  0.3365,  0.9167]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TrafficTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_layers, output_dim=3):\n",
    "        super(TrafficTransformer, self).__init__()\n",
    "\n",
    "        # Embedding layer: Converts location tokens into dense vectors\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "\n",
    "        # Transformer Encoder Layer\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(embed_dim, output_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Convert location indices to dense vectors\n",
    "        x = self.transformer_encoder(x)  # Learn spatial dependencies\n",
    "        x = self.fc(x.mean(dim=1))  # Aggregate features and classify traffic level\n",
    "        return x\n",
    "\n",
    "# Model Hyperparameters\n",
    "input_dim = 10000  # Number of unique location encodings\n",
    "embed_dim = 16     # Size of embedding vectors\n",
    "num_heads = 2      # Number of attention heads\n",
    "num_layers = 2     # Number of Transformer layers\n",
    "output_dim = 3     # 3 Classes: Low, Medium, High Traffic\n",
    "\n",
    "# Initialize model\n",
    "model = TrafficTransformer(input_dim, embed_dim, num_heads, num_layers, output_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Pass tokenized locations into the model\n",
    "output = model(sample_input)\n",
    "\n",
    "print(f\"Transformer Model Predictions (Before Training):\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a222129-6cf2-4018-9e8b-4ece7ea220eb",
   "metadata": {},
   "source": [
    "### 3.3 Training the Transformer Model\n",
    "We train the Transformer model on our dummy dataset to learn traffic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c654ca-a02d-4bf3-a4a2-2cfefae191be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.2254\n",
      "Epoch [2/10], Loss: 0.1982\n",
      "Epoch [3/10], Loss: 0.1971\n",
      "Epoch [4/10], Loss: 0.1774\n",
      "Epoch [5/10], Loss: 0.1752\n",
      "Epoch [6/10], Loss: 0.1740\n",
      "Epoch [7/10], Loss: 0.1549\n",
      "Epoch [8/10], Loss: 0.1681\n",
      "Epoch [9/10], Loss: 0.1557\n",
      "Epoch [10/10], Loss: 0.1500\n",
      "Predicted Traffic Levels: tensor([2, 1, 1, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(sample_input)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(output, traffic_labels)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Predictions after training\n",
    "final_output = model(sample_input)\n",
    "predicted_traffic = torch.argmax(final_output, dim=1)\n",
    "print(f\"Predicted Traffic Levels: {predicted_traffic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ef163a-4482-4417-9ec5-5196a55f1ad3",
   "metadata": {},
   "source": [
    "## 4. Can We Use Raw Latitude and Longitude Values? \n",
    "\n",
    "The answer is YES! \n",
    "\n",
    "Machine learning models require structured data to learn patterns effectively. Geospatial data, which includes latitude and longitude, can be processed in two ways:\n",
    "\n",
    "- Using Raw Latitude and Longitude as numerical features.\n",
    "- Encoding and Tokenizing Locations into structured representations like Geohash, H3, and Plus Codes.\n",
    "\n",
    "Understanding when to use raw lat/lon and when to apply encoding/tokenization is key to improving model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be3fadb-76fa-4a9c-9c84-a559e24817be",
   "metadata": {},
   "source": [
    "### 4.1. When to Use Raw Latitude and Longitude\n",
    "Raw latitude and longitude work well when:\n",
    "\n",
    "#### A. The Model Can Learn Spatial Relationships from Continuous Values\n",
    "Deep learning models (e.g., Transformers, Neural Networks, LSTMs, CNNs) can learn complex spatial patterns from raw lat/lon.\n",
    "These models use positional embeddings, distance functions, and spatial attention mechanisms.\n",
    "Example Use Cases:\n",
    "\n",
    "- Traffic flow prediction: Models can learn spatial dependencies from continuous lat/lon sequences.\n",
    "- Weather forecasting: Models can process lat/lon with temporal sequences to predict temperature, rainfall, etc.\n",
    "- Autonomous navigation: Self-driving cars need exact lat/lon to make precise route calculations.\n",
    "#### B. When Distance-Based Relationships Matter\n",
    "Some problems require understanding the actual distance between points.\n",
    "Raw lat/lon maintains precise distance relationships, while encoding methods group locations into fixed grids, which can reduce accuracy.\n",
    "Example Use Cases:\n",
    "\n",
    "- Delivery route optimization: Distance-based routing requires exact coordinates.\n",
    "- Drone navigation: Flight paths are based on exact GPS coordinates.\n",
    "#### C. The Model is a Regression Model\n",
    "Regression models (e.g., Linear Regression, Neural Networks) can directly process lat/lon as numerical features.\n",
    "Example Use Cases:\n",
    "\n",
    "- Predicting house prices based on location: Raw lat/lon works well when combined with other features like population density and infrastructure.\n",
    "- Estimating environmental pollution: Raw lat/lon can be used with weather data to model pollution spread.\n",
    "#### D. When Using a Transformer Model\n",
    "- Transformers process continuous numerical data well, meaning lat/lon can be input directly.\n",
    "- They learn spatial relationships via self-attention, so additional encoding isn’t always necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d512697e-55cf-441f-ab8a-19a3afb0d2c3",
   "metadata": {},
   "source": [
    "### 4.2. When to Use Encoding and Tokenization\n",
    "Location encoding and tokenization are helpful when:\n",
    "\n",
    "#### A. The Model Works Better with Categorical Data\n",
    "Decision Trees, Random Forests, XGBoost, and Gradient Boosting Models (GBMs) do not process continuous lat/lon well.\n",
    "Encoding methods convert lat/lon into categorical features, making them more suitable for tree-based models.\n",
    "Example Use Cases:\n",
    "\n",
    "- Clustering nearby locations for logistics planning.\n",
    "- Crime rate analysis using Random Forest.\n",
    "- Predicting business success in different regions.\n",
    "#### B. When Spatial Relationships Are More Important Than Exact Distance\n",
    "Encoding groups nearby locations together, making it easier for models to learn regional trends.\n",
    "Raw lat/lon treats every coordinate as unique, making it harder to generalize across locations.\n",
    "Example Use Cases:\n",
    "\n",
    "- Real estate price prediction: Houses in the same area should be grouped, not treated as separate coordinates.\n",
    "- Retail location analysis: Grouping customer locations helps identify market clusters.\n",
    "#### C. The Dataset is Large and Needs Efficient Indexing\n",
    "Encoding methods reduce storage and computation time.\n",
    "Geohash and H3 allow efficient querying of geospatial data.\n",
    "Example Use Cases:\n",
    "\n",
    "- Searching for the nearest coffee shop in an app.\n",
    "- Fast geospatial searches in databases (e.g., Uber, Google Maps).\n",
    "#### D. The Model Needs to Understand Spatial Hierarchies\n",
    "Encodings like H3, Geohash, and Plus Codes provide multi-resolution location representations.\n",
    "They help models understand regions, neighborhoods, and districts.\n",
    "Example Use Cases:\n",
    "\n",
    "- Urban planning and infrastructure development.\n",
    "- Climate analysis at different regional scales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f35cc15-27c4-458a-8621-d20b434379d3",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Location encoding plays a crucial role in transforming geospatial data into structured formats that enhance usability, efficiency, and scalability in digital applications. Whether for navigation, logistics, emergency response, or machine learning, different encoding methods help improve spatial analysis and searchability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02224208-2ebc-4377-8f3c-66874ba62a6c",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "**Raw Latitude and Longitude**\n",
    "\n",
    "Works well for deep learning models (Transformers, Neural Networks) that can learn spatial dependencies.\n",
    "Best suited for applications requiring precise distance calculations, such as navigation, route optimization, and weather prediction.\n",
    "\n",
    "**Encoding and Tokenization (Geohash, H3, Plus Codes, UTM)**\n",
    "\n",
    "Enhances indexing and searchability in large-scale geospatial databases.\n",
    "Improves performance in tree-based models (XGBoost, Random Forest) by converting coordinates into categorical features.\n",
    "Helps in clustering locations and aggregating spatial data, such as crime mapping or urban planning.\n",
    "\n",
    "**Choosing the Right Approach**\n",
    "\n",
    "Use raw lat/lon for continuous spatial data in deep learning models.\n",
    "Use encoding when working with categorical models, spatial indexing, or hierarchical relationships."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoai",
   "language": "python",
   "name": "geoai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
